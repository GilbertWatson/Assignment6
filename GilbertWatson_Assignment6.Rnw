\documentclass{article}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{times}
\usepackage{enumerate}

\textwidth=6.2in
\textheight=8.5in
%\parskip=.3cm
\oddsidemargin=.1in
\evensidemargin=.1in
\headheight=-.3in


%------------------------------------------------------------
% newcommand
%------------------------------------------------------------
\newcommand{\scscst}{\scriptscriptstyle}
\newcommand{\scst}{\scriptstyle}
\newcommand{\Robject}[1]{{\texttt{#1}}}
\newcommand{\Rfunction}[1]{{\texttt{#1}}}
\newcommand{\Rclass}[1]{\textit{#1}}
\newcommand{\Rpackage}[1]{\textit{#1}}
\newcommand{\Rexpression}[1]{\texttt{#1}}
\newcommand{\Rmethod}[1]{{\texttt{#1}}}
\newcommand{\Rfunarg}[1]{{\texttt{#1}}}

\begin{document}
\SweaveOpts{concordance=TRUE}

%------------------------------------------------------------
\title{Assignment 4}
%------------------------------------------------------------
\author{Gilbert Watson}
\date{Tuesday, October 15, 2013}

\SweaveOpts{highlight=TRUE, tidy=TRUE, keep.space=TRUE, keep.blank.space=FALSE, keep.comment=TRUE}
\SweaveOpts{prefix.string=Fig}


\maketitle
\tableofcontents

%-------------------------------------------
\section{5.1}
%--------------------------------------------

For the matrices below, obtain (1) $A + B$, (2) $A - B$, (3) $AC$, (4) $AB'$, (5) $B'A$. State the dimension of each resulting matrix.

\subsection{Answer:}

First, let's define the matricies as in the book:

<<matricies>>=
A <- matrix(data=c(1,4,2,6,3,8),nrow=3,ncol=2,byrow=T)
B <- matrix(data=c(1,3,1,4,2,5),nrow=3,ncol=2,byrow=T)
C <- matrix(data=c(3,8,1,5,4,0),nrow=2,ncol=3,byrow=T)
@

\begin{enumerate}[1)]
\item{} $A + B$:
<<AplusB>>=
answer <- A + B
answer
@
The dimensions are $\Sexpr{dim(A + B)[1]}$ by $\Sexpr{dim(A + B)[2]}$.

\item{} $A - B$:
<<AminusB>>=
answer <- A - B
answer
@
The dimensions are $\Sexpr{dim(A - B)[1]}$ by $\Sexpr{dim(A - B)[2]}$.

\item{} $AC$:
<<AtimesC>>=
answer <- A %*% C
answer
@
The dimensions are $\Sexpr{dim(answer)[1]}$ by $\Sexpr{dim(answer)[2]}$.

\item{} $AB'$:
<<ABprime>>=
answer <- A %*% t(B)
answer
@
The dimensions are $\Sexpr{dim(answer)[1]}$ by $\Sexpr{dim(answer)[2]}$.

\item{} $B'A$:
<<BprimeA>>=
answer <- t(B) %*% A
answer
@
The dimensions are $\Sexpr{dim(answer)[1]}$ by $\Sexpr{dim(answer)[2]}$.

\end{enumerate}

%-------------------------------------------
\section{5.9}
%--------------------------------------------

Let $A$ be defined as in the book:
\begin{enumerate}[a)]
\item{} Are the column vectors of $A$ linearly dependent?
\item{} Restate definition (5.20) in terms of row vectors. Are the row vectors of $A$ linearly dependent?
\item{} What is the rank of $A$?
\item{} Calculate the determinant of $A$.
\end{enumerate}

\subsection{Answer:}

\begin{enumerate}[a)]
\item{} If the determinant of $A$ is non-zero, then the vectors of A are independent. What is the determinant?
<<matrixA>>=
A <- matrix(data=c(0,1,8,0,3,1,0,5,5),nrow=3,ncol=3,byrow=T)
det(A)
@
The determinant of A is zero, therefore, the vectors are dependent.

\item{} Restating definition (5.20) for row vectors:

When c scalars $k_1$, ... , $k_c$, not all zero, can be found such that: 
$$k_1R_1 + k_2R_2 + ... + k_rR_r = 0$$ 
where 0 denotes the zero row vector, the r row vectors are linearly dependent. If the only set of scalars for which the equality holds is $k_1 = 0$, ... , $k_c = 0$, the set of r row vectors is linearly independent.

Let's see if the row vectors are linearly independent. Is the determinant of $A'$ non-zero?

<<det2>>=
det(t(A))
@

The determinant of $A'$ is zero, therefore the rows of A are not linearly independent.

\item{} The rank of $A$ is 2 since any two row vectors can be found to be linearly independent.

\item{} As calculated before, the determinant of A is 0.

\end{enumerate}

%-------------------------------------------
\section{5.26ab}
%--------------------------------------------

Refer to Plastic hardness Problems 1.22 and 5.7.
\begin{enumerate}[a)]
\item{} Using matrix methods, obtain the following: (1) $(X'X)^{-1}$, (2) $b$, (3) $\hat{Y}$, (4) $H$, (5) $SSE$,
(6) $s^2\{b\}$, (7) $s^2\{pred\}$ when $X_h = 30$.
\item{} From part (a), obtain the following: (1) $s^2\{b_0\}$; (2) $s\{b_0,b_1\}$; (3) $s\{b_1\}$.
\end{enumerate}

\subsection{Answer:}

First, let's read in the data.

<<plasticdata>>=
Plastics <- read.table("DataOne.txt")
names(Plastics) <- c("Hardness","Time")
@

\begin{enumerate}[a)]
\item{}
\begin{enumerate}[1)]
\item{} $(X'X)^{-1}$:
<<xxxxxx>>=
# X is time
X <- cbind(rep(1,length(Plastics$Time)),Plastics$Time)
solve(t(X) %*% X)
@
\item{} $b$:
<<getb>>=
A <- solve(t(X) %*% X) %*% t(X)
Y <- Plastics$Hardness
b <- A %*% Y
b
@
\item{} $\hat{Y}$:
<<haty>>=
hatY <- X %*% b
hatY
@
\item{} $H$:
<<HHHHHH>>=
H <- X %*% A
H
@
\item{} $SSE$:
<<SSE>>=
e <- Y - hatY
SSE <- t(e) %*% e
SSE
@
\item{} $s^2\{b\}$:
<<s2things>>=
MSE <- mean(e^2)
s2_b <- MSE*solve(t(X) %*% X)
s2_b
@
\item{} $s^2\{pred\}$:
<<s2pred>>=
X_h <- matrix(data=c(1, 30),nrow=2,ncol=1,byrow=T)
s2_pred <- MSE*(1 + t(X_h) %*% solve(t(X) %*% X) %*% X_h)
s2_pred
@
\end{enumerate}
\item{}
\begin{enumerate}[1)]
\item{} $s^2\{b_0\}$:
<<s2bo>>=
s2_b0 <- s2_b[1,1]
s2_b0
@
\item{} $s\{b_0,b_1\}$:
<<s2b0b1>>=
s_b0b1 <- s2_b[1,2]
s_b0b1
@
\item{} $s\{b_1\}$:
<<s_b1>>=
s_b1 <- sqrt(s2_b[2,2])
s_b1
@
\end{enumerate}
\end{enumerate}

%-------------------------------------------
\section{5.29}
%--------------------------------------------

Consider the least squares estimator $b$ given in (5.60). Using matrix methods, show that $b$ is an
unbiased estimator.

\subsection{Answer:}

The equation for the least squares estimator $b$ is:
$$ b = (X'X)^{-1}X'Y $$
Substitute for $Y$:
$$ b = (X'X)^{-1}X'(\beta{}X + \epsilon{}) $$
Find the expectation of $b$:
$$ E(b) = E((X'X)^{-1}X'(X\beta{} + \epsilon{})) $$
$$ E(b) = E((X'X)^{-1}(X'X)\beta{} + (X'X)^{-1}X'\epsilon{}) $$
Recalling that an assumption of the model is that the errors are uncorrelated with the predictors:
$$ E(b) = \beta{} + E((X'X)^{-1}X')E(\epsilon{}) $$
And that the expectation of the errors is 0:
$$ E(b) = \beta{} $$
Showing that b is unbiased for $\beta$

%-------------------------------------------
\section{6.2}
%--------------------------------------------

Set up the $X$ matrix and $\beta{}$ vector for each of the following regression models (assume $i = 1, ... ,5$):
\begin{enumerate}[a)]
\item{} $Y_i = \beta{}_1X_{i1} + \beta{}_2X_{i2} + \beta{}_3X_{i1}^2 + \epsilon{}_i$;
\item{} $\sqrt{Y_i} = \beta{}_0 + \beta{}_1X_{i1} + \beta{}_2log_{10}X_{i2} + \epsilon{}_i$
\end{enumerate}

\subsection{Answer:}

\begin{enumerate}[a)]
\item{} $$ X = \begin{pmatrix}
                X_{1,1} & X_{1,2} & X_{1,1}^2 \\
                X_{2,1} & X_{2,2} & X_{2,1}^2 \\
                X_{3,1} & X_{3,2} & X_{3,1}^2 \\
                X_{4,1} & X_{4,2} & X_{4,1}^2 \\
                X_{5,1} & X_{5,2} & X_{5,1}^2 \\
                \end{pmatrix} $$
        $$ \beta{} = \begin{pmatrix}
                      \beta{}_1 \\
                      \beta{}_2 \\
                      \beta{}_3 \\
                      \end{pmatrix} $$
\item{} $$ X = \begin{pmatrix}
                1 & X_{1,1} & log_{10}X_{1,2} \\
                1 & X_{2,1} & log_{10}X_{2,2} \\
                1 & X_{3,1} & log_{10}X_{3,2} \\
                1 & X_{4,1} & log_{10}X_{4,2} \\
                1 & X_{5,1} & log_{10}X_{5,2} \\
                \end{pmatrix} $$
        $$ \beta{} = \begin{pmatrix}
                      \beta{}_0 \\
                      \beta{}_1 \\
                      \beta{}_2 \\
                      \end{pmatrix} $$
\end{enumerate}

%-------------------------------------------
\section{6.22}
%--------------------------------------------

For each of the following regression models, indicate whether it is a general linear regression model. If it is not, state whether it can be expressed in the form of (6.7) by a suitable transformation:
\begin{enumerate}[a)]
\item{} $ Y_i = \beta{}_0 + \beta{}_1X_{i1} + \beta{}_2log_{10}X_{i2} + \beta{}_3X_{i1}^2 + \epsilon{}_i $
\item{} $ Y_i = \epsilon{}_iexp(\beta{}_0 + \beta{}_1X_{i1} + \beta{}_2X_{i2}^2) $
\item{} $ Y_i = log_{10}(\beta{}_1X_{i1}) + \beta{}_2X_{i2} + \epsilon{}_i $
\item{} $ Y_i = \beta{}_0 exp(\beta{}_1X_{i1}) + \epsilon{}_i $
\item{} $ Y_i = [1 + exp(\beta{}_0 + \beta{}_1X_{i1} + \epsilon{}_i)]^{-1} $
\end{enumerate}

\subsection{Answer:}

\begin{enumerate}[a)]
\item{} Yes, the model is a GLM.
\item{} No, and there is no suitable transformation. $Y' = ln(Y)$ appears to be a potential candidate, but the term $ln(\epsilon)$ would remain - this would not be a GLM.
\item{} No, and there is no suitable transformation.
\item{} No, and there is no suitable transformation.
\item{} No, but there is a suitable transformation: $Y' = ln(Y^{-1} - 1)$.
\end{enumerate}

%-------------------------------------------
\section{6.27}
%--------------------------------------------

In a small-scale regression study, the following data were obtained:

\begin{tabular}{c c c c c c c}
$i$: & 1 & 2 & 3 & 4 & 5 & 6 \\ \hline
$X_{i1}$: & 7 & 4 & 16 & 3 & 21 & 8 \\
$X_{i2}$: & 33 & 41 & 7 & 49 & 5 & 31 \\
$Y_i$: & 42 & 33 & 75 & 28 & 91 & 55 \\
\end{tabular}

Assume that regression model (6.1) with independent normal error terms is appropriate. Using matrix methods, obtain (a) $b$; (b) $e$; (c) $H$; (d) $SSR$; (e) $s^2\{b\}$; (f) $\hat{Y}_h$ when $X_{h1} = 10$, $X_{h2} = 30$; (g) $s^2\{\hat{Y}_h\}$ when $X_{h1} = 10$, $X_{h2} = 30$.

\subsection{Answer:}

First, let's construct the data:

<<makingthedata>>=
X <- cbind(rep(1,6),
           c(7,4,16,3,21,8),
           c(33,41,7,49,5,31))
Y <- matrix(c(42,33,75,28,91,55),nrow=6,ncol=1,byrow=T)
@

\begin{enumerate}[a)]
\item{} $b$:
<<getbagain>>=
A <- solve(t(X) %*% X) %*% t(X)
b <- A %*% Y
b
@
\item{} $e$:
<<finde>>=
hatY <- X %*% b
e <- Y - hatY
e
@
\item{} $H$:
<<H>>=
H <- X %*% A
H
@
\item{} $SSR$:
<<SSR>>=
J <- matrix(1,nrow=6,ncol=6)
SSR <- t(Y) %*% (H - (1/6)*J) %*% Y
SSR
@
\item{} $s^2\{b\}$:
<<agains2b>>=
MSE <- mean(e^2)
s2_b <- MSE*solve(t(X) %*% X)
s2_b
@
\item{} $\hat{Y}_h$ when $X_h1 = 10$ and $X_h2 = 30$:
<<anotherinference>>=
X_h <- matrix(c(1,10,30),nrow=3,ncol=1,byrow=T)
hatY_h <- t(X_h) %*% b
hatY_h
@
\item{} $s^2\{\hat{Y}_h\}$ when $X_h1 = 10$ and $X_h2 = 30$:
<<mores2sssssssss>>=
s2hatY_h <- t(X_h) %*% s2_b %*% X_h
s2hatY_h
@
\end{enumerate}

%-------------------------------------------
\section{System Information}
%--------------------------------------------

<<sessionInfo>>=
sessionInfo();
@ 

\end{document}