\documentclass{article}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{times}
\usepackage{enumerate}

\textwidth=6.2in
\textheight=8.5in
%\parskip=.3cm
\oddsidemargin=.1in
\evensidemargin=.1in
\headheight=-.3in


%------------------------------------------------------------
% newcommand
%------------------------------------------------------------
\newcommand{\scscst}{\scriptscriptstyle}
\newcommand{\scst}{\scriptstyle}
\newcommand{\Robject}[1]{{\texttt{#1}}}
\newcommand{\Rfunction}[1]{{\texttt{#1}}}
\newcommand{\Rclass}[1]{\textit{#1}}
\newcommand{\Rpackage}[1]{\textit{#1}}
\newcommand{\Rexpression}[1]{\texttt{#1}}
\newcommand{\Rmethod}[1]{{\texttt{#1}}}
\newcommand{\Rfunarg}[1]{{\texttt{#1}}}

\begin{document}
\SweaveOpts{concordance=TRUE}

%------------------------------------------------------------
\title{Assignment 6}
%------------------------------------------------------------
\author{Gilbert Watson}
\date{Monday, November 4th, 2013}

\SweaveOpts{highlight=TRUE, tidy=TRUE, keep.space=TRUE, keep.blank.space=FALSE, keep.comment=TRUE}
\SweaveOpts{prefix.string=Fig}


\maketitle
\tableofcontents

%-------------------------------------------
\section{6.5}
%--------------------------------------------

In a small-scale experimental study of the relation between degree of brand liking ($Y$) and moisture content ($X_1$) and sweetness ($X_2$) of the product, the following results were obtained from the experiment based on a completely randomized design (data are coded):

\begin{enumerate}[a)]
\item{} Obtain the scatter plot matrix and the correlation matrix. What information do these diagnostic aids provide here?
\item{} Fit regession model (6.1) to the data. State the estimated regression function. How is $b_1$ interpreted here?
\item{} Obtain the residuals and prepare a box plot of the residuals. What information does this plot provide?
\item{} Plot the residuals against $Y$, $X_1$, $X_2$ , and $X_1$ $X_2$ on separate graphs. Also prepare a normal probability plot. Interpret the plots and summarize your findings.
\item{} Conduct the Breusch-Pagan test for constancy of the error variance, assuming $log(\sigma{}^2) = \gamma{}_0 + \gamma{}_1X_{i1} + \gamma{}_2X_{i2}$: use $\alpha{} = .01$. State the alternaltives, decision rule, and conclusion.
\end{enumerate}

\subsection{Answer:}

\begin{enumerate}[a)]
\item{} First let's load the data:

<<loadit>>=
BrandPreference <- read.table(file="6.5.txt",stringsAsFactors=F)
names(BrandPreference) <- c("Brand.Liking","Moisture.Content","Sweetness")
@

Then let's generate the scatter plot matrix:

<<scatterplot,fig=TRUE>>=
require(ggplot2)
require(reshape2)
melted <- melt(BrandPreference,id.vars=c("Brand.Liking"))
g <- ggplot(data=melted,aes(Brand.Liking,value))
g + geom_point() + facet_wrap(~variable)
@

And now let's compute the correlation matrix:

<<correlationmatrix>>=
cor(BrandPreference)
@

\item{} Now let's estimate the model $Y_i = b_0 + b_1X_{i1} + b_2X_{i2} + \epsilon{}_i$:
<<makemodel>>=
fit <- lm(Brand.Liking~Moisture.Content+Sweetness,data=BrandPreference)
fitsum <- summary(fit)
fitsum
@

So, the estimated model is:
$$ Y_i = \Sexpr{fitsum$coefficients["(Intercept)","Estimate"]} + \Sexpr{fitsum$coefficients["Moisture.Content","Estimate"]}X_{i1} + \Sexpr{fitsum$coefficients["Sweetness","Estimate"]}X_{i2} $$

\item{} Now let's examine the residuals:

<<resids,fig=TRUE>>=
boxplot(fit$residuals)
@

This plot shows that the residuals are roughly normally distributed around 0.

\item{} Let's plot the residuals against $Y$, $X_1$, $X_2$ , and $X_1$ $X_2$:
 
<<moreplots, fig=TRUE>>=
qplot(x=fit$residuals,
      y=fit$model$Brand.Liking,
      xlab="Residuals",
      ylab="Brand Liking")
@

<<moreplots2, fig=TRUE>>=
qplot(x=fit$residuals,
      y=fit$model$Moisture.Content,
      xlab="Residuals",
      ylab="Moisture Content")
@

<<moreplots3, fig=TRUE>>=
qplot(x=fit$residuals,
      y=fit$model$Sweetness,
      xlab="Residuals",
      ylab="Sweetness")
@

<<moreplots4, fig=TRUE>>=
qplot(x=fit$residuals,
      y=fit$model$Moisture.Content*fit$model$Sweetness,
      xlab="Residuals",ylab="Moisture Content times Sweetness")
@

<<moreplots5, fig=TRUE>>=
qplot(sample=fit$residuals,
      stat="qq",
      main="Normal Probability Plot for Model Residuals")
@

The plots indicate the the residuals are not only distributed normally, but are uncorrelated with either any of the predictors or the outcome. The model assumptions hold.

\item{} Let's perform the Breush-Pagan test for heteroskedasticity of the error terms

<<bptest>>=
require(lmtest)
pb <- bptest(fit,studentize=F)
pb
@

Clearly, we cannot reject the null hypothesis that the model exhibits homoskedasticity.

\end{enumerate}

%-------------------------------------------
\section{7.3}
%--------------------------------------------

Refer to Brand Preference Problem 6.5:

\begin{enumerate}[a)]
\item{} Obtain the analysis of variance table that decomposes the regression sum of squares into extra sums of squares associated with $X_1$, and with $X_2$ , given $X_1$.
\item{} Test whether $X_2$ can be dropped from the regression model given that $X_1$ is retained. Use the $F^*$ test statistic and level of significance .01. State the alternatives, decision rule, and conclusion. What is the P-value of the test?
\end{enumerate}

\subsection{Answer:}

\begin{enumerate}[a)]
\item{} First we must fit the first order model:

<<ppsumsq>>=
fo <- lm(Brand.Liking~Moisture.Content,data=BrandPreference)
fosum <- summary(fo)
fosum
@

Now we can find $SSR(X_2|X_1)$:

<<ssrx2x1>>=
ssr_x2x1 <- deviance(fo) - deviance(fit)
ssr_x2x1
@

So, the extra sum of squares, $SSR(X_2|X_1) = \Sexpr{ssr_x2x1}$.

\item{} Now let's test to see if we can drop $X_2$ given $X_1$. The alternatives are as follows:

$$ H_0: \beta{}_3 = 0 $$
$$ H_a: \beta{}_3 \neq{} 0 $$

The decision rule is reject $H_0$ if:

$$ F^* > F(0.99,1,\Sexpr{length(BrandPreference$Brand.Liking) - fitsum$df[1]}) $$

Let's compute the test statistic and critical value:

<<testfstar>>=
# test statistic
F_star <- ((deviance(fo) - deviance(fit)) /
  ((length(BrandPreference$Brand.Liking) - fosum$df[1]) - 
     (length(BrandPreference$Brand.Liking) - fitsum$df[1]))) /
  (deviance(fit)/(length(BrandPreference$Brand.Liking) - fitsum$df[1]))
# critical value
cv <- qf(0.99,1,length(BrandPreference$Brand.Liking)-fitsum$df[1])
print(paste0("F^star is ",F_star," and the critical value is ",cv,"."))
@

Clearly we reject $H_0$ and conclude that $X_2$ should remain in the model. Let's compute the p-value of the test statistic:

<<getp>>=
pval <- 1 - pf(F_star,1,length(BrandPreference$Brand.Liking)-fitsum$df[1])
print(paste0("The p-value for F^star is: ",pval))
@

\end{enumerate}

%-------------------------------------------
\section{7.12}
%--------------------------------------------

Refer to Brand Preference Problem 6.5: Calculate $R_{Y1}^2$, $R_{Y2}^2$, $R_{|2}^2$, $R_{Y1|2}^2$, $R_{Y2|1}^2$, and $R^2$. Explain what each coefficient measures and interpret your results.

\subsection{Answer:}

\begin{enumerate}[a)]

\item{} $R_{Y1}^2$:

<<y1>>=
anovafit <- anova(fit)
r2_y1 <- anovafit["Moisture.Content","Sum Sq"]/sum(anovafit$`Sum Sq`)
r2_y1
@

\item{} $R_{Y2}^2$:

<<y2>>=
r2_y2 <- anovafit["Sweetness","Sum Sq"]/sum(anovafit$`Sum Sq`)
r2_y2
@

\item{} $R_{|2}^2$:

\item{} $R_{Y1|2}^2$:

<<y12>>=
fo2 <- lm(Brand.Liking~Sweetness,data=BrandPreference)
fosum2 <- summary(fo2)
r2_y12 <- (deviance(fo2) - deviance(fit))/deviance(fo2)
r2_y12
@

\item{} $R_{Y2|1}^2$:

<<y21>>=
r2_y21 <- (deviance(fo) - deviance(fit))/deviance(fo)
r2_y21
@

\item{} $R^2$:

<<r2>>=
r2 <- fitsum$r.squared
r2
@

\end{enumerate}

%-------------------------------------------
\section{7.24}
%--------------------------------------------

Refer to Brand Preference Problem 6.5:

\begin{enumerate}[a)]
\item{} Fit first-order simple linear regression model (2.1) for relating brand liking ($Y$) to moisture content ($X_1$). State the fitted regression function.
\item{} Compare the estimated regression coefficient for moisture content obtained in part (a) with the corresponding coefficient obtained in Problem 6.5b. What do you find?
\item{} Does $SSR(X_1)$ equal $SSR(X_1|X_2)$ here? If not, is the difference substantial?
\item{} Refer to the correlation matrix obtained in Problem 6.5a. What bearing does this have on your findings in parts (b) and (c)?
\end{enumerate}

\subsection{Answer:}

\begin{enumerate}[a)]
\item{} Let's first fit the first order model:

<<firstorder>>=
fo <- lm(Brand.Liking~Moisture.Content,data=BrandPreference)
fosum <- summary(fo)
fosum
@

The estimated function is:
$$ Y_i = \Sexpr{fosum$coefficients["(Intercept)","Estimate"]} + \Sexpr{fosum$coefficients["Moisture.Content","Estimate"]}X_{i1} $$

\item{} Let's compare the estimated models:

<<compare>>=
# two predictor model
fitsum
# one predictor model
fosum
@

We find that the coefficients are the same in both models for Sweetness.

\item{} Let's find $SSR(X_1|X_2)$ and $SSR(X_1)$:

<<ssrssssssss>>=
SSR_x1x2 <- deviance(fo2) - deviance(fit)
SSR_x1 <- sum(anova(fo)$`Sum Sq`) - deviance(fo)
print(paste0("SSR(X1|X2) is ",SSR_x1x2," and SSR(X1) is ",SSR_x1))
@

Clearly $SSR(X_1|X_2)$ and $SSR(X_1)$ are both the same.

\item{} As suggested by the correlation matrix, Moisture Content and Sweetness are uncorrelated, so we would expect the estimated coefficients from part (b) to be the same for both models. We would also expect the $SSR(X_1|X_2)$ to be the same as $SSR(X_1)$ since $X_1$ and $X_2$ are unrelated. The presence or absence of $X_2$ provides no information about $X_1$. The converse is also true.

\end{enumerate}

%-------------------------------------------
\section{7.28}
%--------------------------------------------

\begin{enumerate}[a]
\item{} Define each of the following exta sums of squares: (I) $SSR(X_5|X_1)$, (2) $SSR(X_3,X_4|X_1)$, (3) $SSR(X_4|X_1,X_2,X_3)$.
\item{} For a multiple regression model with five $X$ variables, what is the relevant exta sum of squares for testing whether or not $\beta{}_5 = 0$? whether or not $\beta{}_2 = \beta{}_4 = 0$?
\end{enumerate}

\subsection{Answer:}

\begin{enumerate}[a)]
\item{}
\begin{enumerate}[1)]
\item{} $$ SSR(X_5|X_1) = SSR(X_1,X_5) - SSR(X_1) $$
\item{} $$ SSR(X_3,X_5|X_1) = SSR(X_1,X_3,X_5) - SSR(X_1) $$
\item{} $$ SSR(X_4|X_1,X_2,X_3) = SSR(X_1,X_2,X_3,X_4) - SSR(X_1,X_2,X_3) $$
\end{enumerate}
\item{} The relevant sum of squares for whether or not $\beta{}_5 = 0$:
$$ SSR(X_5|X_1,X_2,X_3,X_4,X_5) $$
The relevant sum of squares for whether or not $\beta{}_2 = \beta{}_4 = O$:
$$ SSR(X_2,X_4|X_1,X_3,X_5) $$
\end{enumerate}

%-------------------------------------------
\section{7.29}
%--------------------------------------------

Show that:
\begin{enumerate}[a)]
\item{} $SSR(X_1,X_2,X_3,X_4) = SSR(X_1) + SSR(X_2,X_3|X_1) + SSR(X4|X_1,X_2,X_3)$.
\item{} $SSR(X_1,X_2,X_3,X_4) = SSR(X_2,X_3) + SSR(X_1|X_2,X_3) + SSR(X4|X_1,X_2,X_3)$.
\end{enumerate}

\subsection{Answer:}

\begin{enumerate}[a)]
\item{} $$ SSR(X_1) + SSR(X_2,X_3|X_1) + SSR(X4|X_1,X_2,X_3) $$
$$ = SSR(X_1) + SSR(X_1,X_2,X_3) - SSR(X_1) + SSR(X_1,X_2,X_3,X_4) - SSR(X_1,X_2,X_3) $$
$$ = SSR(X_1,X_2,X_3,X_4) $$
as required.
\item{} $$ SSR(X_2,X_3) + SSR(X_1|X_2,X_3) + SSR(X4|X_1,X_2,X_3) $$
$$ = SSR(X_2,X_3) + SSR(X_1,X_2,X_3) - SSR(X_2,X_3) + SSR(X_1,X_2,X_3,X_4) - SSR(X_1,X_2,X_3) $$
$$ = SSR(X_1,X_2,X_3,X_4) $$
as required.
\end{enumerate}

%-------------------------------------------
\section{7.30}
%--------------------------------------------

Refer to Brand Preference Problem 6.5:

\begin{enumerate}[a)]
\item{} Regress $Y$ on $X_2$ using simple linear regression model (2.1) and obtain the residuals.
\item{} Regress $X_1$ on $X_2$ using simple linear regression model (2.1) and obtain the residuals.
\item{} Calculate the coefficient of simple correlation between the two sets of residuals and show that it equals $R_{Y1|2}$.
\end{enumerate}

\subsection{Answer:}

\begin{enumerate}[a)]
\item{} We have already previously estimated this model, so we can obtain the residuals:

<<morebrands>>=
res_fo2 <- residuals(fo2)
print(res_fo2)
@

\item{} Now let's regress $X_1$ on $X_2$ and obtain the residuals:

<<morebrands2>>=
xonx <- lm(Moisture.Content~Sweetness,data=BrandPreference)
res_xonx <- residuals(xonx)
print(res_xonx)
@

\item{} The simple correlation coefficient between the residuals for the two models is:

<<corresids>>=
cor(x=res_fo2,y=res_xonx)
@

Now let's show that is is roughly equal to $r_{Y1|2}$:

<<roughlyequal>>=
r_y12 <- sqrt((deviance(fo2) - deviance(fit))/deviance(fo2))
r_y12
@

Which is clearly the same as the coefficient of simple correlation between the two sets of residuals.

\end{enumerate}

%-------------------------------------------
\section{8.11}
%--------------------------------------------

Refer to Brand Preference Problem 6.5:

\begin{enumerate}[a)]
\item{} Fit regression model (8.22).
\item{} Test whether or not the interaction term can be dropped from the model; use $\alpha{} = .05$. State the alternatives, decision rule, and conclusion.
\end{enumerate}

\subsection{Answer:}

\begin{enumerate}[a)]
\item{} Let's fit the model:

<<yetanotherbpodel>>=
wint <- lm(Brand.Liking~Moisture.Content+Sweetness+Moisture.Content*Sweetness,
           data=BrandPreference)
wint_sum <- summary(wint)
wint_sum
@

\item{} Now let's test and see if the interaction term can be removed from the model. The alternatives are as follows:

$$ H_0: \beta{}_4 = 0 $$
$$ H_a: \beta{}_4 \neq{} 0 $$

The decision rule is reject $H_0$ if:

$$ F^* > F(0.95,1,\Sexpr{length(BrandPreference$Brand.Liking) - wint_sum$df[1]}) $$

Let's compute the test statistic and critical value:

<<testfstaragain>>=
# test statistic
F_star <- ((deviance(fit) - deviance(wint)) /
  ((length(BrandPreference$Brand.Liking) - fitsum$df[1]) - 
     (length(BrandPreference$Brand.Liking) - wint_sum$df[1]))) /
  (deviance(wint)/(length(BrandPreference$Brand.Liking) - wint_sum$df[1]))
# critical value
cv <- qf(0.95,1,length(BrandPreference$Brand.Liking)-wint_sum$df[1])
print(paste0("F^star is ",F_star," and the critical value is ",cv,"."))
@

Clearly we fail to reject $H_0$ and conclude that $X_1X_2$ should not remain in the model. Let's compute the p-value of the test statistic:

<<getpagain>>=
pval <- 1 - pf(F_star,1,length(BrandPreference$Brand.Liking)-wint_sum$df[1])
print(paste0("The p-value for F^star is: ",pval))
@

\end{enumerate}

%-------------------------------------------
\section{8.16}
%--------------------------------------------

Refer to Grade point average Problem 1.19. An assistant to the director of admissions conjectured that the predictive power of the model could be improved by adding information on whether the student had chosen a major field of concentration at the time the application was submitted. Assume that regression model (8.33) is appropriate, where $X_1$ is entrance test score and $X_2 = 1$ if student had indicated a major field of concentration at the time of application and 0 if the major field was undecided. 

\begin{enumerate}[a)]
\item{} Explain how each regression coefficient in model (8.33) is interpreted here.
\item{} Fit the regression model and state the estimated regression function.
\item{} Test whether the $X_2$ variable can be dropped from the regression model; use $\alpha{} = .01$. State the alternatives. decision rule. and conclusion.
\item{} Obtain the residuals for regression model (8.33) and plot them against $X_1X_2$. Is there any evidence in your plot that it would be helpful to include an interaction term in the model?
\end{enumerate}

\subsection{Answer:}

\begin{enumerate}[a)]
\item{} $\beta{}_1$ can be interpreted as the effect of a one unit increase in entrance test score - a one unit increase in entrance test score yeilds a $\beta{}_1$ unit increase in a student's first year GPA, on average. $\beta{}_2$ can be interpreted as the average difference in GPA points associated with students who stated thier major field at the time of application irrespective of entrance test score.
\item{} Fit the model:

<<schools>>=
GPA <- read.table(file='8.16.txt',stringsAsFactors=F)
names(GPA) <- c("GPA","Entrance.Test","Major.Indicated")
gpafit <- lm(GPA~Entrance.Test+Major.Indicated,data=GPA)
gpafit_sum <- summary(gpafit)
gpafit_sum
@

The estimated model is:
$$ Y_i = \Sexpr{gpafit_sum$coefficients["(Intercept)","Estimate"]} + \Sexpr{gpafit_sum$coefficients["Entrance.Test","Estimate"]}X_{i1} + \Sexpr{gpafit_sum$coefficients["Entrance.Test","Estimate"]}X_{i2} $$

\item{} Now let's test and see if $X_2$ can be removed from the model. The alternatives are as follows:

$$ H_0: \beta{}_3 = 0 $$
$$ H_a: \beta{}_3 \neq{} 0 $$

The decision rule is reject $H_0$ if:

$$ F^* > F(0.99,1,\Sexpr{length(GPA$GPA) - gpafit_sum$df[1]}) $$

Let's compute the test statistic and critical value:

<<testfstaragain2>>=
# test statistic
gpafo <- lm(GPA~Entrance.Test,data=GPA)
gpafo_sum <- summary(gpafo)
F_star <- ((deviance(gpafo) - deviance(gpafit)) /
  ((length(GPA$GPA) - gpafo_sum$df[1]) - 
     (length(GPA$GPA) - gpafit_sum$df[1]))) /
  (deviance(gpafit)/(length(GPA$GPA) - gpafit_sum$df[1]))
# critical value
cv <- qf(0.99,1,length(GPA$GPA)-gpafit_sum$df[1])
print(paste0("F^star is ",F_star," and the critical value is ",cv,"."))
@

Clearly we fail to reject $H_0$ and conclude that $X_2$ should not remain in the model.

\item{} Let's plot the residuals vs the variable $X_1X_2$:

<<moreresplots, fig=TRUE>>=
qplot(x=residuals(gpafit),
      y=Entrance.Test*Major.Indicated,
      data=GPA,
      xlab="Residuals",
      ylab="Entrance Test Score times Indicator Variable for Major Declaration")
@

There appears to be a positive relationship between the residuals and $X_1X_2$ when $X_2 = 0$. It may be beneficial to include the term.

\end{enumerate}

%-------------------------------------------
\section{8.20}
%--------------------------------------------

Refer to Grade point average Problems 1.19 and 8.16:

\begin{enumerate}[a)]
\item{} Fit regression model (8.49) and state the estimated regression function.
\item{} Test whether the interaction term can be dropped from the model; use $\alpha{} = .05$. State the alternatives. decision rule, and conclusion. If the interaction term cannot be dropped from the model, describe the nature of the interaction effect.
\end{enumerate}

\subsection{Answer:}

\begin{enumerate}[a)]
\item{} Fit the model:

<<interactionmodel>>=
intfit <- lm(GPA~Entrance.Test+Major.Indicated+Entrance.Test*Major.Indicated,
             data=GPA)
intfit_sum <- summary(intfit)
intfit_sum
@

The estimated model is:
$$ Y_i = \Sexpr{intfit_sum$coefficients["(Intercept)","Estimate"]} + \Sexpr{intfit_sum$coefficients["Entrance.Test","Estimate"]}X_{i1} + \Sexpr{intfit_sum$coefficients["Major.Indicated","Estimate"]}X_{i2} + \Sexpr{intfit_sum$coefficients["Entrance.Test:Major.Indicated","Estimate"]}X_{i1}X_{i2} $$

\item{} Let's test whether the interaction term is beneficial to the model. The alternatives are as follows:

$$ H_0: \beta{}_4 = 0 $$
$$ H_a: \beta{}_4 \neq{} 0 $$

The decision rule is reject $H_0$ if:

$$ F^* > F(0.95,1,\Sexpr{length(GPA$GPA) - gpafit_sum$df[1]}) $$

Let's compute the test statistic and critical value:

<<testfstaragain2>>=
# test statistic
F_star <- ((deviance(gpafit) - deviance(intfit)) /
  ((length(GPA$GPA) - gpafit_sum$df[1]) - 
     (length(GPA$GPA) - intfit_sum$df[1]))) /
  (deviance(intfit)/(length(GPA$GPA) - intfit_sum$df[1]))
# critical value
cv <- qf(0.95,1,length(GPA$GPA)-intfit_sum$df[1])
print(paste0("F^star is ",F_star," and the critical value is ",cv,"."))
@

Clearly we reject $H_0$ and conclude that $X_1X_2$ should remain in the model. The nature of the interaction effect is that if an incoming student declared a major at time of admittance, then the marginal effect of a one point increase in entrance test score on first year GPA is $\Sexpr{intfit_sum$coefficients["Entrance.Test:Major.Indicated","Estimate"]}$ higher than the baseline marginal effect when a student doesn't initially indicate a major.

\end{enumerate}

%-------------------------------------------
\section{8.42}
%--------------------------------------------

Refer to Market share data set in Appendix C.3. Company executives want to be able to predict market share of their product ($Y$) based on merchandise price ($X_1$), the gross Nielsen rating points ($X_2$, an index of the amount of advertising exposure that the product received); the presence or absence of a wholesale pricing discount ($X_3 = 1$ if discount present: otherwise $X_3 = 0$); the presence or absence of a package promotion during the period ($X_4 = 1$ if promotion present: otherwise $X_4 = 0$): and year ($X_5$). Code year as a nominal level variable and use 2000 as the reference year.

\begin{enumerate}[a)]
\item{} Fit a first-order regression model. Plot the residuals against the fitted values. How well does the first-order model appear to fit the data?
\item{} Re-fit the model in part (a). After adding all second-order terms involving only the quantitative predictors. Test whether or not all quadratic and interaction terms can be dropped from the regression model: use $\alpha = .05$. State the alternatives. decision rule, and conclusion.
\item{} In part (a), test whether advertising index ($X_2$) and year ($X_5$) can be dropped from the model; use $\alpha = .05$. State the alternatives, decision rule, and conclusion.
\end{enumerate}

\subsection{Answer:}

\begin{enumerate}[a)]
\item{} First, let's read in the data and recode the year variable into an indicator with a base of the year 2000:

<<readdata>>=
MarketShare <- read.table('MarketShare.txt',stringsAsFactors=F)
names(MarketShare) <- c("id","Market.Share","Merchandise.Price","Nielson.Rating","Wholesale.Price.Discount","Package.Promotion","Month","Year")
MarketShare$i1999 <- 1*(MarketShare$Year == 1999)
MarketShare$i2001 <- 1*(MarketShare$Year == 2001) 
MarketShare$i2002 <- 1*(MarketShare$Year == 2002)
@

Now fit the first order regression model and plot the residuals:

<<fstorderregmod, fig=TRUE>>=
msfit <- lm(Market.Share~Merchandise.Price+Nielson.Rating+Wholesale.Price.Discount+Package.Promotion+i1999+i2001+i2002,
            data = MarketShare)
msfit_sum <- summary(msfit)
msfit_sum

# plot the residuals
qplot(x=msfit$fitted.values,y=msfit$residuals,xlab="Fitted Values",ylab="Residuals",main="Residuals vs. Fitted Values")
@

The first order model appears to fit reasonably well. The residuals appear to have no relationship with the fitted values. The size of the residuals may get larger with increasing size of the fitted values, but it is not clear that this is the case.

\item{} Now let's refit with quadratic and interaction terms for the quantitative predictors.

<<refitititititi>>=
msfitint <- lm(Market.Share~Merchandise.Price+Nielson.Rating+I(Merchandise.Price^2)+I(Nielson.Rating^2)+
                 Merchandise.Price*Nielson.Rating+Wholesale.Price.Discount+Package.Promotion+
                 i1999+i2001+i2002,
            data = MarketShare)
msfitint_sum <- summary(msfitint)
msfitint_sum
@

Now let's test to see if these added terms should be removed. The alternatives are as follows:

$$ H_0: \beta{}_4 = \beta{}_5 = \beta{}_6 = 0 $$
$$ H_a: \beta{}_4 \neq{} 0 or \beta{}_5 \neq{} 0 or \beta{}_6 \neq{} 0 $$

The decision rule is reject $H_0$ if:

$$ F^* > F(0.95,1,\Sexpr{length(MarketShare$id) - msfitint_sum$df[1]}) $$

Let's compute the test statistic and critical value:

<<testfstaragain2>>=
# test statistic
F_star <- ((deviance(msfit) - deviance(msfitint)) /
  ((length(MarketShare$id) - msfit_sum$df[1]) - 
     (length(MarketShare$id) - msfitint_sum$df[1]))) /
  (deviance(msfitint)/(length(MarketShare$id) - msfitint_sum$df[1]))
# critical value
cv <- qf(0.95,1,length(MarketShare$id)-msfitint_sum$df[1])
print(paste0("F^star is ",F_star," and the critical value is ",cv,"."))
@

Clearly, we fail to reject $H_0$ and conclude that we should drop quadratic and interaction terms from the model.

\item{} Now let's test to see if we can remove year indicators and the Nielson rating: The alternatives are as follows:

$$ H_0: \beta{}_3 = \beta{}_6 = \beta{}_7 = \beta{}_8 = 0 $$
$$ H_a: \beta{}_3 \neq{} 0 or \beta{}_6 \neq{} 0 or \beta{}_7 \neq{} 0 or \beta{}_8 \neq{} 0$$

The decision rule is reject $H_0$ if:

$$ F^* > F(0.95,1,\Sexpr{length(MarketShare$id) - msfit_sum$df[1]}) $$

Let's compute the test statistic and critical value. We must also fit a reduced model:

<<testfstaragain2>>=
# reduced model:
msfitrd <- lm(Market.Share~Merchandise.Price+Wholesale.Price.Discount+Package.Promotion,
            data = MarketShare)
msfitrd_sum <- summary(msfitrd)

# test statistic
F_star <- ((deviance(msfitrd) - deviance(msfit)) /
  ((length(MarketShare$id) - msfitrd_sum$df[1]) - 
     (length(MarketShare$id) - msfit_sum$df[1]))) /
  (deviance(msfit)/(length(MarketShare$id) - msfit_sum$df[1]))
# critical value
cv <- qf(0.95,1,length(MarketShare$id)-msfit_sum$df[1])
print(paste0("F^star is ",F_star," and the critical value is ",cv,"."))
@

Clearly, we fail to reject $H_0$ and conclude that we should drop year indicators and the Nielson rating. This model estimation follows:

<<finalmodel>>=
msfitrd_sum
@

\end{enumerate}


%-------------------------------------------
\section{System Information}
%--------------------------------------------

<<sessionInfo>>=
sessionInfo();
@ 

\end{document}